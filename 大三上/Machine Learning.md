## Algorithm

Traditional machine learning algorithm

### 回归

* 线性回归 

$$
y = ax+b
$$

求解回归系数$(a,b)$ ，并使误差最小



### 分类

* 逻辑回归 ：找一条直线来分类数据
  * 通过Sigmoid函数将线性函数的结果映射到Sigmoid函数中，预估事件出现的概率并分类
  * Sigmoid：归一化函数，将连续型数据离散化为离散型数据

* K-近邻 ：用距离度量最相邻的分类标签

  * 计算样本数据中的点与当前点之间的距离
  * 算法提取样本最相似数据(最近邻)的分类标签
  * 确定前k个点所在类别的出现频率. 一般只选择样本数据集中前k个最相似的数据，这就是k-近邻算法中k的出处，通常k是不大于20的整数
  * 返回前k个点所出现频率最高的类别作为当前点的预测分类

* 朴素贝叶斯 ：选择后验概率最大的类为分类标签

  * P(X|C): 条件概率，C中X出现的概率
    P(C): 先验概率，C出现的概率
    P(C|X): 后验概率，X属于C类的概率

    假设有 C1 和 C2 两个类，由于 P(X)都是一样的，所以不需要考虑 P(X) 只需考虑如下：
    如果 P(X|C1)P(C1) > P(X|C2)P(C2)，则 P(C1|X) > P(C2|X)，得 X 属于C1；
    如果 P(X|C1) P(C1) < P(X|C2) P(C2)，则 P(C2|X) < P(C2|X)，得 X 属于C2。

  * 主要应用：文本分类、垃圾文本过滤，情感判别，多分类实时预测等

* 决策树 ：构造一棵熵值下降最快的分类树

  * 一种树型结构，其中每个内部结点表示在一个属性上的测试，每个分支代表一个测试输出，每个叶结点代表一种类别。采用的是自顶向下的递归方法，选择信息增益（信息增益率）最大的特征作为当前的分裂特征。
  * 常用于：用户分级评估、贷款风险评估、选股、投标决策等。

* 支持向量机(**SVM**) ：构造超平面，分类非线性数据

  * 当一个分类问题，数据是线性可分时，只要将线的位置放在让小球距离线的距离最大化的位置即可，寻找这个最大间隔的过程，就叫做最优化。
  * 一般的数据是线性不可分的，可以通过核函数，将数据从二维映射到高位，通过超平面将数据切分。
  * 不同方向的最优决策面的分类间隔通常是不同的，那个具有“最大间隔”的决策面就是SVM要寻找的最优解。这个真正的最优解对应的两侧虚线所穿过的样本点，就是SVM中的支持样本点，称为支持向量。
  * 常用于垃圾邮件识别、手写识别、文本分类、选股等。

### 聚类

* K-means ：计算质心，聚类无标签数据
  * 随机生成k个初始点作为质心；
  * 将数据集中的数据按照距离质心的远近分到各个簇中；
  * 将各个簇中的数据求平均值，作为新的质心，重复上一步，直到所有的簇不再改变。 两个分类间隔越远，则聚类效果越好。

### 关联分析

* FP-growth
  * 频繁项集：在数据集中大量频繁出现的数据集合
  * 关联规则：由集合A，可以在某置信度下推出集合B，$A\rightarrow B$
  * 支持度：某频繁项集在数据集中的比例
  * 置信度：若$(A,B)支持度x，(A)支持度为y$，则$A\rightarrow B$置信度为$\frac{x}{y}$

### 降维

* PCA降维 ：减少数据维度，降低数据复杂度 (principle component analysis)
  * 通过某种线性投影，将高维的数据映射到低维的空间中表示，并期望在所投影的维度上数据的方差最大，以此使用较少的数据维度，同时保留住较多的原数据点的特性。



### 人工神经网路

逐层抽象，逼近任意函数

### 深度学习